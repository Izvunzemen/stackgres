apiVersion: v1
kind: Namespace
metadata:
  labels:
    app: stackgres-operator
    control-plane: stackgres-controller-manager
    app.kubernetes.io/name: namespace
    app.kubernetes.io/instance: system
    app.kubernetes.io/component: manager
    app.kubernetes.io/created-by: stackgres
    app.kubernetes.io/part-of: stackgres
    app.kubernetes.io/managed-by: kustomize
  name: system
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: controller-manager
  namespace: system
  labels:
    app: stackgres-operator
    control-plane: stackgres-controller-manager
    app.kubernetes.io/name: deployment
    app.kubernetes.io/instance: controller-manager
    app.kubernetes.io/component: manager
    app.kubernetes.io/created-by: stackgres
    app.kubernetes.io/part-of: stackgres
    app.kubernetes.io/managed-by: kustomize
spec:
  selector:
    matchLabels:
      app: stackgres-operator
      control-plane: stackgres-controller-manager
  replicas: 1
  template:
    metadata:
      annotations:
        kubectl.kubernetes.io/default-container: manager
      labels:
        app: stackgres-operator
        control-plane: stackgres-controller-manager
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                - key: kubernetes.io/arch
                  operator: In
                  values:
                    - amd64
                    - arm64
                - key: kubernetes.io/os
                  operator: In
                  values:
                    - linux
      securityContext:
        runAsNonRoot: true
        # TODO(user): For common cases that do not require escalating privileges
        # it is recommended to ensure that all your Pods/Containers are restrictive.
        # More info: https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted
        # Please uncomment the following code if your project does NOT have to work on old Kubernetes
        # versions < 1.19 or on vendors versions which do NOT support this field by default (i.e. Openshift < 4.11 ).
        # seccompProfile:
        #   type: RuntimeDefault
      containers:
      - args:
        - --leader-elect
        - --leader-election-id=stackgres
        image: controller:latest
        imagePullPolicy: IfNotPresent
        name: manager
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
              - "ALL"
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8081
          initialDelaySeconds: 15
          periodSeconds: 20
        readinessProbe:
          httpGet:
            path: /readyz
            port: 8081
          initialDelaySeconds: 5
          periodSeconds: 10
        # TODO(user): Configure the resources accordingly based on the project requirements.
        # More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
        resources:
          limits:
            cpu: 1
            memory: 512Mi
          requests:
            cpu: 10m
            memory: 128Mi
      initContainers:
      - command:
        - sh
        - -ec
        - |
          OPERATOR_NAME="\$(kubectl get operators.operators.coreos.com -o name \
            | grep '^operator.operators.coreos.com/stackgres' \
            | head -n 1 | cut -d / -f 2)"
          test -n "\$OPERATOR_NAME"
          OPERATOR_UID="\$(kubectl get operators.operators.coreos.com "\$OPERATOR_NAME" --template '{{ .metadata.uid }}')"
          test -n "\$OPERATOR_UID"
          PREVIOUS_SGCONFIG_NAME="\$(kubectl get sgconfig -o name | cut -d / -f 2)"
          PREVIOUS_SGCONFIG_SPEC=""
          if [ "\$PREVIOUS_SGCONFIG_NAME" != stackgres-operator ] \
            && kubectl get sgconfig "\$PREVIOUS_SGCONFIG_NAME" -o yaml | grep -q '^spec:\$' \
            && kubectl get sgconfig "\$PREVIOUS_SGCONFIG_NAME" -o yaml | grep -q '^status:\$'
          then
            # TODO: Remove this as soon as version 1.4 get out of support!
            PREVIOUS_SGCONFIG="\$(kubectl get sgconfig "\$PREVIOUS_SGCONFIG_NAME" -o yaml)"
            PREVIOUS_SGCONFIG_SPEC_LINE="\$(printf %s "\$PREVIOUS_SGCONFIG" \
              | grep -n '^spec:\$' | cut -d : -f 1)"
            PREVIOUS_SGCONFIG_STATUS_LINE="\$(printf %s "\$PREVIOUS_SGCONFIG" \
              | grep -n '^status:\$' | cut -d : -f 1)"
            PREVIOUS_SGCONFIG_SPEC="\$(printf %s "\$PREVIOUS_SGCONFIG" \
              | head -n "\$((PREVIOUS_SGCONFIG_STATUS_LINE - 1))" \
              | tail -n +"\$((PREVIOUS_SGCONFIG_SPEC_LINE + 1))")"
            kubectl delete sgconfig "\$PREVIOUS_SGCONFIG_NAME"
            kubectl delete clusterrole -l "meta.helm.sh/release-name=\$PREVIOUS_SGCONFIG_NAME"
            kubectl delete clusterrolebinding -l "meta.helm.sh/release-name=\$PREVIOUS_SGCONFIG_NAME"
            kubectl delete validatingwebhookconfiguration -l "meta.helm.sh/release-name=\$PREVIOUS_SGCONFIG_NAME"
            kubectl delete mutatingwebhookconfiguration -l "meta.helm.sh/release-name=\$PREVIOUS_SGCONFIG_NAME"
          fi
          cat << EOF | kubectl apply -f -
          apiVersion: stackgres.io/v1
          kind: SGConfig
          metadata:
            name: stackgres-operator
            ownerReferences:
            - apiVersion: operators.coreos.com/v1
              kind: Operator
              name: \$OPERATOR_NAME
              uid: \$OPERATOR_UID
          spec:
          \$PREVIOUS_SGCONFIG_SPEC
          EOF
        image: ongres/kubectl:v1.25.5-build-6.19
        imagePullPolicy: IfNotPresent
        name: default
        securityContext:
          runAsNonRoot: true
$(
if [ "$OPENSHIFT_BUNDLE" != true ]
then
  cat << EOF
          runAsUser: 1000
          runAsGroup: 1000
EOF
fi
)
          allowPrivilegeEscalation: false
          capabilities:
            drop:
              - "ALL"
        # TODO(user): Configure the resources accordingly based on the project requirements.
        # More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
        resources:
          limits:
            cpu: 1
            memory: 512Mi
          requests:
            cpu: 10m
            memory: 128Mi
      serviceAccountName: controller-manager
      terminationGracePeriodSeconds: 10
