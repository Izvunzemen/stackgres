#!/bin/sh

. "$SPEC_PATH/abstract/patroni"

. "$SPEC_PATH/abstract/sql-scripts"

. "$SPEC_PATH/abstract/backup"

. "$SPEC_PATH/abstract/dbops-restart"

e2e_test_extra_hash() {
  "$SHELL" "$PROJECT_PATH/stackgres-k8s/ci/build/build-functions.sh" path_hash \
    "$(realpath --relative-to "$PROJECT_PATH" "$SPEC_PATH/abstract/patroni")"
  "$SHELL" "$PROJECT_PATH/stackgres-k8s/ci/build/build-functions.sh" path_hash \
    "$(realpath --relative-to "$PROJECT_PATH" "$SPEC_PATH/abstract/sql-scripts")"
  "$SHELL" "$PROJECT_PATH/stackgres-k8s/ci/build/build-functions.sh" path_hash \
    "$(realpath --relative-to "$PROJECT_PATH" "$SPEC_PATH/abstract/backup")"
  "$SHELL" "$PROJECT_PATH/stackgres-k8s/ci/build/build-functions.sh" path_hash \
    "$(realpath --relative-to "$PROJECT_PATH" "$SPEC_PATH/abstract/dbops-restart")"
  "$SHELL" "$PROJECT_PATH/stackgres-k8s/ci/build/build-functions.sh" path_hash \
    "$(realpath --relative-to "$PROJECT_PATH" "$SPEC_PATH/sql-scripts.sakila.sql")"
}

e2e_test_install() {
  CLUSTER_NAME="$(get_sgshardedcluster_name "$SPEC_NAME")"

  install_minio

  cat << 'EOF' | kubectl create -n "$CLUSTER_NAMESPACE" secret generic sql-scripts-sakila-user \
    --from-literal=create-sakila-user.sql="$(cat)"
DO $$
BEGIN
  IF NOT EXISTS (SELECT * FROM pg_roles WHERE rolname = 'sakila') THEN
    EXECUTE 'CREATE USER sakila WITH PASSWORD ''sakila'';';
  END IF;
  IF NOT EXISTS (SELECT * FROM pg_dist_authinfo WHERE rolename = 'sakila') THEN
    INSERT INTO pg_dist_authinfo (nodeid, rolename, authinfo) VALUES (0, 'sakila', 'password=sakila');
  END IF;
END$$;
EOF

  kubectl create -n "$CLUSTER_NAMESPACE" configmap sql-scripts-sakila-schema \
    --from-file=create-sakila-schema.sql="$SPEC_PATH/sql-scripts.sakila.sql"

  create_or_replace_sharded_cluster "$CLUSTER_NAME" "$CLUSTER_NAMESPACE" "3" "2"

  deploy_curl_pod "$CLUSTER_NAMESPACE"

  wait_pods_running "$CLUSTER_NAMESPACE" 8
  wait_sharded_cluster "$CLUSTER_NAME" "$CLUSTER_NAMESPACE"
}

e2e_test() {
  run_test "Checking that is possible to connect using services is working" service_check

  run_test "Checking that sharded technology is configured and working" sharded_check

  run_test "Check that pgbouncer database is accesible using the service" pgbouncer_database_check

  run_test "Checking that managed SQL is working" check_managed_sql_is_working

  run_test "Checking that backup is working" check_backup_is_working

  run_test "Checking that sharded backup are working" check_sharded_backup_is_working

  run_test "Checking that automatic sharded backup are working" check_automatic_sharded_backup_is_working

  run_test "Checking that restoring a sharded backup is working" check_restore_sharded_backup_is_working

  run_test "Checking that resharding sharded dbops is working" check_sharded_dbops_resharding_is_working

  run_test "Checking that restart sharded dbops is working" check_sharded_dbops_restart_is_working
}

service_check() {
  RESPONSE_PRIMARY="$(run_query -c "$CLUSTER_NAME-coord" -h "$CLUSTER_NAME" -i 1 -p 5432)"

  if [ "$RESPONSE_PRIMARY" = "1" ]
  then
    echo "SUCCESS: Connections are possible using services"
  else
    echo "FAIL: Cannot connect to primary db using a kubernetes service"
    return 1
  fi
}

sharded_check() {
  local RESULT EXIT_CODE
  try_function wait_until eval 'run_query -c "$CLUSTER_NAME-coord" -h "$CLUSTER_NAME" -i 1 -p 5432 \
    -d citus -q "SELECT COUNT(*) FROM master_get_active_worker_nodes()" | grep -xF 2'

  if [ "$EXIT_CODE" = 0 ]
  then
    echo "SUCCESS: Sharding coordinator service is working"
  else
    echo "FAIL: Sharding coordinator service is not working"
    return 1
  fi

  try_function wait_until eval 'run_query -c "$CLUSTER_NAME-coord" -h "$CLUSTER_NAME" -i 1 -p 5432 \
    -d citus -q "SELECT COUNT(*) FROM master_get_active_worker_nodes()" | grep -xF 2'

  if [ "$EXIT_CODE" = 0 ]
  then
    echo "SUCCESS: Sharding primary coordinator service is working"
  else
    echo "FAIL: Sharding primary coordinator service is not working"
    return 1
  fi

  try_function wait_until eval 'run_query -c "$CLUSTER_NAME-coord" -h "$CLUSTER_NAME-shards" -i 1 -p 5432 \
    -d citus -q "SELECT pg_is_in_recovery()" | grep -xF f'

  if [ "$EXIT_CODE" = 0 ]
  then
    echo "SUCCESS: Sharding shards service is working"
  else
    echo "FAIL: Sharding shards service is not working"
    return 1
  fi

  try_function wait_until eval 'run_query -c "$CLUSTER_NAME-coord" -h "$CLUSTER_NAME-reads" -i 1 -p 5432 \
    -d citus -q "SELECT setting FROM pg_settings WHERE name = '"'"'citus.max_client_connections'"'"'" | grep -xF 30'

  if [ "$EXIT_CODE" = 0 ]
  then
    echo "SUCCESS: Setting citus.max_client_connections is correctly configured"
  else
    echo "FAIL: Setting citus.max_client_connections is not correctly configured"
    return 1
  fi
}

pgbouncer_database_check() {
  if kubectl exec -n "$CLUSTER_NAMESPACE" "${CLUSTER_NAME}-coord-0" -c "postgres-util" -- env \
    PGPASSWORD="$(kubectl -n "$CLUSTER_NAMESPACE" get secrets "$CLUSTER_NAME-coord" \
      -o jsonpath='{.data.pgbouncer-admin-password}' | base64 -d)" \
    PGCONNECT_TIMEOUT="$((5 + E2E_TIMEOUT / 10))" \
    psql -t -A -U pgbouncer_admin -d pgbouncer -h "$CLUSTER_NAME" -c "SHOW FDS" >/dev/null
  then
    echo "SUCCESS: psql could connect to the pgbouncer database with pgobuncer_admin using service"
  else
    echo "FAIL: psql could not connect to the pgbouncer database with pgobuncer_admin using service"
    return 1
  fi

  if kubectl exec -n "$CLUSTER_NAMESPACE" "${CLUSTER_NAME}-coord-0" -c "postgres-util" -- env \
    PGPASSWORD="$(kubectl -n "$CLUSTER_NAMESPACE" get secrets "$CLUSTER_NAME-coord" \
      -o jsonpath='{.data.pgbouncer-stats-password}' | base64 -d)" \
    PGCONNECT_TIMEOUT="$((5 + E2E_TIMEOUT / 10))" \
    psql -t -A -U pgbouncer_stats -d pgbouncer -h "$CLUSTER_NAME" -c "SHOW VERSION" >/dev/null
  then
    echo "SUCCESS: psql could connect to the pgbouncer database with pgobuncer_stats using service"
  else
    echo "FAIL: psql could not connect to the pgbouncer database with pgobuncer_stats using service"
    return 1
  fi
}

check_managed_sql_is_working() {
  local NODE=0
  local DATABASE=citus
  check_user_on_sharded_primary
  check_database_on_sharded_primary
  check_schema_on_sharded_primary
}

check_backup_is_working() {
  local BACKUP_NAME
  local SHARDED_CLUSTER_NAME="$CLUSTER_NAME"
  local CLUSTER_NAME
  local NODE=0
  local SUFFIX
  for SUFFIX in "coord" "shard0" "shard1"
  do
    CLUSTER_NAME="$SHARDED_CLUSTER_NAME-$SUFFIX"
    check_wal_archive 0
  done
}

check_sharded_backup_is_working() {
  SHARDED_BACKUP_NAME="$(get_sgshardedbackup_name "${CLUSTER_NAME}-$(shuf -i 0-65535 -n 1)")"

  create_sharded_backup "$SHARDED_BACKUP_NAME" true

  local BACKUP_NAME
  local SHARDED_CLUSTER_NAME="$CLUSTER_NAME"
  local CLUSTER_NAME
  local NODE=0
  local SUFFIX
  for SUFFIX in coord shard0 shard1
  do
    CLUSTER_NAME="$SHARDED_CLUSTER_NAME-$SUFFIX"
    BACKUP_NAME="$SHARDED_BACKUP_NAME-$SUFFIX"
    wait_backup_is_completed "$BACKUP_NAME" "$NODE"
  done

  wait_sharded_backup_is_completed "$SHARDED_BACKUP_NAME"

  if kubectl get sgshardedbackup -n "$CLUSTER_NAMESPACE" "$SHARDED_BACKUP_NAME" \
    --template='{{ range .status.sgBackups }}{{ . }} {{ end }}' \
    | grep -qFx "$SHARDED_BACKUP_NAME-coord $SHARDED_BACKUP_NAME-shard0 $SHARDED_BACKUP_NAME-shard1 "
  then
    echo "SUCCESS: Backups are referenced correctly in the sharded backup"
  else
    echo "FAIL: Backups are not referenced correctly in the sharded backup"
    return 1
  fi

  CLUSTER_NAME="$SHARDED_CLUSTER_NAME"
  SHARDED_BACKUP_NAME_1="$(get_sgshardedbackup_name "${CLUSTER_NAME}-$(shuf -i 0-65535 -n 1)")"
  SHARDED_BACKUP_NAME_2="$(get_sgshardedbackup_name "${CLUSTER_NAME}-$(shuf -i 0-65535 -n 1)")"
  create_sharded_backup "$SHARDED_BACKUP_NAME_1" true
  wait_sharded_backup_is_completed "$SHARDED_BACKUP_NAME_1"
  create_sharded_backup "$SHARDED_BACKUP_NAME_2" true
  wait_sharded_backup_is_completed "$SHARDED_BACKUP_NAME_2"

  if ! kubectl get sgshardedbackup -n "$CLUSTER_NAMESPACE" "$SHARDED_BACKUP_NAME" >/dev/null 2>&1
  then
    echo "SUCCESS: Sharded backups retention policy is working"
  else
    echo "FAIL: Sharded backups retention policy is not working"
    return 1
  fi
}

check_automatic_sharded_backup_is_working() {
  enable_sharded_cluster_cron_schedule

  if wait_until is_automatic_sharded_backup_cr_completed
  then
    echo "SUCCESS. The full automatic sharded backup is available"
  else
    echo "FAIL. The full automatic sharded backup is not available"
    return 1
  fi

  local BACKUP_NAMES

  BACKUP_NAMES="$(kubectl get sgshardedbackup -n "$CLUSTER_NAMESPACE" \
    --template '{{ range .items }}{{ .status.process.status }} {{ .spec.managedLifecycle }} {{ .spec.sgShardedCluster }} {{ range .status.sgBackups }}{{ . }} {{ end }}{{ print "\n" }}{{ end }}' \
    | grep "^Completed true ${CLUSTER_NAME} " | tail -n 1 | cut -d ' ' -f 4-)"

  if [ -z "$BACKUP_NAMES" ]
  then
    echo "FAIL. Can not retrieve backups associated to the full automatic sharded backup"
    return 1
  fi

  local BACKUP_NAME
  local SHARDED_CLUSTER_NAME="$CLUSTER_NAME"
  local CLUSTER_NAME
  local NODE=0
  for BACKUP_NAME in $BACKUP_NAMES
  do
    if CLUSTER_NAME="$(kubectl get sgbackup -n "$CLUSTER_NAMESPACE" "$BACKUP_NAME" \
      --template='{{ .status.process.status }} {{ .spec.sgCluster }}' \
      | grep '^Completed ')"
    then
      CLUSTER_NAME="$(printf %s "$CLUSTER_NAME" | cut -d ' ' -f 2)"

      check_timelines

      check_control_data

      echo "SUCCESS. The full backup $BACKUP_NAME is available"
    else
      echo "FAIL. The full backup $BACKUP_NAME is not available"
      return 1
    fi
  done

  CLUSTER_NAME="$SHARDED_CLUSTER_NAME"
  disable_sharded_cluster_cron_schedule
}


create_sharded_backup() {
  cat << EOF | kubectl create -f -
apiVersion: stackgres.io/v1
kind: SGShardedBackup
metadata:
  namespace: "$CLUSTER_NAMESPACE"
  name: "$1"
spec:
  sgShardedCluster: "$CLUSTER_NAME"
  managedLifecycle: $2
EOF
}

wait_sharded_backup_is_completed() {
  local BACKUP_NAME="$1"
  if wait_until eval '[ "$(kubectl get sgshardedbackup -n "$CLUSTER_NAMESPACE" "$BACKUP_NAME" \
    --template "{{ .status.process.status }}" \
    | grep "^Completed$" | wc -l)" -gt 0 ]'
  then
    echo "SUCCESS. The sharded backup CR has complete"
  else
    echo "FAIL. The sharded backup CR has failed"
    return 1
  fi
}

enable_sharded_cluster_cron_schedule() {
  # Sets the value At every minute.
  kubectl patch sgshardedcluster -n "$CLUSTER_NAMESPACE" "$CLUSTER_NAME" --type json \
    --patch '[{"op":"replace","path":"/spec/configurations/backups/0/cronSchedule","value":"*/1 * * * *"}]'
}

disable_sharded_cluster_cron_schedule() {
  # Sets the value At 05:00 on day-of-month 31 in February.
  kubectl patch sgshardedcluster -n "$CLUSTER_NAMESPACE" "$CLUSTER_NAME" --type json \
    --patch '[{"op":"replace","path":"/spec/configurations/backups/0/cronSchedule","value":"0 5 31 2 *"}]'
}

is_automatic_sharded_backup_cr_completed() {
  kubectl get sgshardedbackup -n "$CLUSTER_NAMESPACE" \
    --template '{{ range .items }}{{ .status.process.status }} {{ .spec.managedLifecycle }} {{ .spec.sgShardedCluster }}{{ print "\n" }}{{ end }}' \
    | grep -q "^Completed true ${CLUSTER_NAME}$"
}

check_restore_sharded_backup_is_working() {
  SHARDED_BACKUP_NAME="$(kubectl get sgshardedbackup -n "$CLUSTER_NAMESPACE" \
    --template '{{ range .items }}{{ printf "%s %s\n" .status.process.status .metadata.name }}{{ end }}' \
    | grep "^Completed " | head -n 1 | cut -d ' ' -f 2)"

  kubectl get secret -n "$CLUSTER_NAMESPACE" "$CLUSTER_NAME" -o json \
    | jq "del(.metadata.ownerReferences) | del(.metadata.labels) | .metadata.name = \"$CLUSTER_NAME-back\"" \
    | kubectl create -f -

  remove_sharded_cluster "$CLUSTER_NAME" "$CLUSTER_NAMESPACE"

  kubectl get secret -n "$CLUSTER_NAMESPACE" "$CLUSTER_NAME-back" -o json \
    | jq ".metadata.name = \"$CLUSTER_NAME\"" \
    | kubectl create -f -

  create_or_replace_sharded_cluster "$CLUSTER_NAME" "$CLUSTER_NAMESPACE" "3" "2" \
    --set-string cluster.initialData.restore.fromBackup.name="$SHARDED_BACKUP_NAME"

  local RESULT EXIT_CODE
  try_function wait_pods_running "$CLUSTER_NAMESPACE" 8

  if "$RESULT"
  then
    try_function wait_sharded_cluster "$CLUSTER_NAME" "$CLUSTER_NAMESPACE"
  fi

  if "$RESULT"
  then
    echo "SUCCESS. The sharded cluster has been recovered from the sharded backup"
  else
    echo "FAIL. The sharded cluster has not been recovered from the sharded backup"
    return 1
  fi
}

check_sharded_dbops_resharding_is_working() {
  SHARDED_DBOPS_NAME="$(get_sgshardeddbops_name resharding-"${CLUSTER_NAME}-$(shuf -i 0-65535 -n 1)")"

  NODE_NAME="$(kubectl exec -n "$CLUSTER_NAMESPACE" "${CLUSTER_NAME}-coord-0" -c "postgres-util" -- \
    psql -t -A -d citus -c "SELECT nodename from pg_dist_node WHERE shouldhaveshards LIMIT 1")"
  kubectl exec -n "$CLUSTER_NAMESPACE" "${CLUSTER_NAME}-coord-0" -c "postgres-util" -- \
    psql -d citus -v ON_ERROR_STOP=1 \
    -c "CREATE TABLE distributed AS SELECT i, (SELECT string_agg(md5(random()::text), ' ') FROM generate_series(1, (30 * random())::integer)) AS t FROM generate_series(1, 100000) AS i" \
    -c "ALTER TABLE distributed ADD PRIMARY KEY (i)"
  wait_until kubectl exec -n "$CLUSTER_NAMESPACE" "${CLUSTER_NAME}-coord-0" -c "postgres-util" -- \
    psql -d citus -v ON_ERROR_STOP=1 \
    -c "SELECT create_distributed_table('distributed', 'i')"
  kubectl exec -n "$CLUSTER_NAMESPACE" "${CLUSTER_NAME}-coord-0" -c "postgres-util" -- \
    psql -d citus -v ON_ERROR_STOP=1 \
    -c "SELECT citus_drain_node('$NODE_NAME', 7433)" \
    -c "SELECT citus_remove_node('$NODE_NAME', 7433)" \
    -c "SELECT citus_add_node('$NODE_NAME', 7433)"
  if ! kubectl exec -n "$CLUSTER_NAMESPACE" "${CLUSTER_NAME}-coord-0" -c "postgres-util" -- \
    psql -t -A -d citus -c "SELECT COUNT(*) FROM get_rebalance_table_shards_plan()" | grep -qxF 0
  then
    echo "SUCCESS. The sharded cluster requires resharding"
  else
    echo "FAIL. The sharded cluster does not requires resharding"
    return 1
  fi

  cat << EOF | kubectl create -f -
apiVersion: stackgres.io/v1
kind: SGShardedDbOps
metadata:
  name: $SHARDED_DBOPS_NAME
  namespace: $CLUSTER_NAMESPACE
spec:
  sgShardedCluster: $CLUSTER_NAME
  op: resharding
  resharding:
    citus:
      threshold: 0.1
      drainOnly: false
      rebalanceStrategy: by_disk_size
EOF

  wait_sharded_dbops_is_completed "$SHARDED_DBOPS_NAME"

  if kubectl exec -n "$CLUSTER_NAMESPACE" "${CLUSTER_NAME}-coord-0" -c "postgres-util" -- \
    psql -t -A -d citus -c "SELECT COUNT(*) FROM get_rebalance_table_shards_plan()" | grep -qxF 0
  then
    echo "SUCCESS. The sharded cluster has been resharded"
  else
    echo "FAIL. The sharded cluster has not been resharded"
    return 1
  fi
}

check_sharded_dbops_restart_is_working() {
  SHARDED_DBOPS_NAME="$(get_sgshardeddbops_name restart-"${CLUSTER_NAME}-$(shuf -i 0-65535 -n 1)")"

  cat << EOF | kubectl create -f -
apiVersion: stackgres.io/v1
kind: SGShardedDbOps
metadata:
  name: $SHARDED_DBOPS_NAME
  namespace: $CLUSTER_NAMESPACE
spec:
  sgShardedCluster: $CLUSTER_NAME
  op: restart
  restart:
    method: InPlace
EOF

  local DBOPS_NAME
  local SUFFIX
  for SUFFIX in coord shard0 shard1
  do
    DBOPS_NAME="$SHARDED_DBOPS_NAME-$SUFFIX"
    wait_dbops_is_completed "$DBOPS_NAME"
  done

  wait_sharded_dbops_is_completed "$SHARDED_DBOPS_NAME"
}

wait_dbops_is_completed() {
  local DBOPS_NAME="$1"
  if wait_until eval 'kubectl get sgdbops -n "$CLUSTER_NAMESPACE" "$DBOPS_NAME" \
    --template "{{ range .status.conditions }}{{ if eq .status \"True\" }}{{ .type }}{{ end }}{{ end }}" \
    | grep -q "^Completed$"'
  then
    echo "SUCCESS. The dbops has completed"
  else
    echo "FAIL. The dbops has failed or did not completed"
    return 1
  fi
}

wait_sharded_dbops_is_completed() {
  local SHARDED_DBOPS_NAME="$1"
  if wait_until eval 'kubectl get sgshardeddbops -n "$CLUSTER_NAMESPACE" "$SHARDED_DBOPS_NAME" \
    --template "{{ range .status.conditions }}{{ if eq .status \"True\" }}{{ .type }}{{ end }}{{ end }}" \
    | grep -q "^Completed$"'
  then
    echo "SUCCESS. The sharded dbops has completed"
  else
    echo "FAIL. The sharded dbops has failed or did not completed"
    return 1
  fi
}
